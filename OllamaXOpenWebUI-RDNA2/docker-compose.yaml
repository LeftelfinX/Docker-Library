networks:
  AI_network:
    name: AI_network
    driver: bridge

services:
  # Ollama Runtime : Running on port 11434
  ollama:
    image: ollama/ollama:rocm
    container_name: ollama-container
    # labels:
    #   - "com.centurylinklabs.watchtower.enable=true"
    restart: always
    devices:
      - /dev/kfd
      - /dev/dri
    environment:
      - HSA_OVERRIDE_GFX_VERSION=10.3.0 # Change if not RDNA2
      - HCC_AMDGPU_TARGET=gfx1030 # Change if not RDNA2
    volumes:
      - ${HOME}/.Docker-Volumes/ollama_data:/root/.ollama
    networks:
      - AI_network
    ports:
      - "127.0.0.1:11434:11434"

  # WEB Interface : Can be accessed from any machine on port 6500
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    # labels:
    #   - "com.centurylinklabs.watchtower.enable=true"
    restart: always
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434 # Use service name instead of 127.0.0.1
    volumes:
      - ${HOME}/.Docker-Volumes/open-webui:/app/backend/data
    networks:
      - AI_network
    ports:
      - "6500:8080"

volumes:
  ollama:
  open-webui:
